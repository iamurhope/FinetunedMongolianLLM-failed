# üá≤üá≥ Fine-tuned Mongolian LLM (Llama-2-7B + LoRA + Unsloth)

**Llama-2-7B —Å—É—É—Ä—å –∑–∞–≥–≤–∞—Ä—ã–≥ –º–æ–Ω–≥–æ–ª —Ö—ç–ª—ç—ç—Ä instruction –¥–∞–≥–∞—Ö, –∞—Å—É—É–ª—Ç-—Ö–∞—Ä–∏—É–ª—Ç –∑—ç—Ä—ç–≥ NLP –¥–∞–∞–ª–≥–∞–≤–∞—Ä—Ç –∑–æ—Ä–∏—É–ª–∂ Parameter-efficient (PEFT) –∞—Ä–≥–∞–∞—Ä LoRA –±–æ–ª–æ–Ω Unsloth –∞—à–∏–≥–ª–∞–Ω fine-tune —Ö–∏–π—Å—ç–Ω —Ç”©—Å”©–ª.**

---

## ‚ú® –û–Ω—Ü–ª–æ–≥

- üß† **–°—É—É—Ä—å –∑–∞–≥–≤–∞—Ä:** Llama-2-7B (Meta AI)
- ‚öôÔ∏è **–°—É—Ä–≥–∞–ª—Ç—ã–Ω –∞—Ä–≥–∞:** LoRA + 4-bit Quantization
- üöÄ **Framework:** Unsloth (Colab optimized)
- üìö **Dataset —Ñ–æ—Ä–º–∞—Ç:** Alpaca (`instruction`, `input`, `output`)
- üéØ **–ó–æ—Ä–∏–ª–≥–æ:** –ú–æ–Ω–≥–æ–ª —Ö—ç–ª–Ω–∏–π LLM-–Ω –∑–∞–∞–≤–∞—Ä –¥–∞–≥–∞—Ö –±–æ–ª–æ–Ω QA —á–∞–Ω–∞—Ä—ã–≥ —Å–∞–π–∂—Ä—É—É–ª–∞—Ö

---

## ‚öôÔ∏è –ó–∞–≥–≤–∞—Ä—ã–Ω —Ç–æ—Ö–∏—Ä–≥–æ–æ

```python
model_name = "meta-llama/Llama-2-7b-hf"
max_seq_length = 2048
lora_config = {
  "r": 16,
  "alpha": 16,
  "target_modules": [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
  ],
  "load_in_4bit": True
}
```

---

## üöÄ –°—É—Ä–≥–∞–ª—Ç—ã–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä“Ø“Ø–¥

```python
training_args = {
  "per_device_train_batch_size": 2,
  "gradient_accumulation_steps": 4,
  "warmup_steps": 5,
  "num_train_epochs": 6,
  "learning_rate": 2e-4,
  "optimizer": "adamw_8bit",
  "weight_decay": 0.01,
  "lr_scheduler_type": "linear",
  "save_steps": 100,
  "save_total_limit": 3
}
```

---

## üì¶ –°—É—É—Ä–∏–ª—É—É–ª–∞—Ö (Google Colab)

```bash
pip install -q -U transformers accelerate peft bitsandbytes datasets huggingface_hub
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft
```

### Google Drive —Ö–æ–ª–±–æ—Ö:
```python
from google.colab import drive
drive.mount('/content/drive')
```

---

## üß† Dataset –∂–∏—à—ç—ç

```json
{
  "instruction": "–•–∏–π–º—ç–ª –æ—é—É–Ω —É—Ö–∞–∞–Ω –≥—ç–∂ —é—É –≤—ç?",
  "input": null,
  "output": "–•–∏–π–º—ç–ª –æ—é—É–Ω —É—Ö–∞–∞–Ω –±–æ–ª –∫–æ–º–ø—å—é—Ç–µ—Ä–∏–π–Ω —à–∏–Ω–∂–ª—ç—Ö —É—Ö–∞–∞–Ω—ã —Å–∞–ª–±–∞—Ä –±”©–≥”©”©–¥ ”©–≥”©–≥–¥–ª”©”©—Å —Å—É—Ä–∞–ª—Ü–∞–∂, –ª–æ–≥–∏–∫ —à–∏–π–¥–≤—ç—Ä –≥–∞—Ä–≥–∞—Ö —á–∞–¥–≤–∞—Ä—Ç–∞–π —É—Ö–∞–∞–ª–∞–≥ —Å–∏—Å—Ç–µ–º —é–º."
}
```

---

## ‚ö†Ô∏è ”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω–≥–∏–π–Ω —á–∞–Ω–∞—Ä—ã–Ω —Ç–æ–º –∞—Å—É—É–¥–∞–ª

| –ê—Å—É—É–¥–∞–ª | –¢–∞–π–ª–±–∞—Ä |
|---|---|
| ‚ùå `input` = float64 | –¢–µ–∫—Å—Ç –±–∞–π—Ö —ë—Å—Ç–æ–π —Ç–∞–ª–±–∞—Ä—Ç —Å–∞–Ω–∞–º—Å–∞—Ä–≥“Ø–π **—Ç–æ–æ** –æ—Ä—Å–æ–Ω |
| ‚ùå –ê–≤—Ç–æ –æ—Ä—á—É—É–ª–≥–∞ | –ê–Ω–≥–ª–∏‚Üí–ú–æ–Ω–≥–æ–ª –æ—Ä—á—É—É–ª–≥–∞ **–¥“Ø—Ä–º–∏–π–Ω –±–æ–ª–æ–Ω —É—Ç–≥—ã–Ω –∏—Ö –∞–ª–¥–∞–∞—Ç–∞–π** |
| ‚ùå Wrong facts | –¢“Ø“Ø—Ö, –≥–∞–∑–∞—Ä–∑“Ø–π, —à–∏–Ω–∂–ª—ç—Ö —É—Ö–∞–∞–Ω—ã **–±–∞—Ä–∏–º—Ç —Ö—É–¥–∞–ª/–∑”©—Ä—á–∏–ª—Ç—ç–π** |
| ‚ùå Mixed languages | –î–∞—Ç–∞–¥ **–º–æ–Ω–≥–æ–ª, –∞–Ω–≥–ª–∏, —Ö—è—Ç–∞–¥** —Ö–æ–ª–∏–≥–¥—Å–æ–Ω |
| ‚ùå Hallucination –∏—Ö | –•–∞—Ä–∏—É–ª—Ç—É—É–¥ –ª–æ–≥–∏–∫ –±–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç **—Ç–∞—Å–∞—Ä—Å–∞–Ω, —Ö–∏–π “Ø–∑—ç–≥–¥—ç–ª—Ç—ç–π** |

### –ù”©–ª”©”©:
```
üëâ –°—É—É—Ä—å Llama-2-—ã–Ω –º—ç–¥–ª—ç–≥ –º—É—É –¥–∞—Ç–∞–¥ —Ö—ç—Ç overfit —Ö–∏–π—Å–Ω—ç—ç—Å  
   ‚ÄúCatastrophic Forgetting + Bad Overfitting‚Äù “Ø“Ø—Å—ç–∂, –±—É—Ä—É—É —Ö–∞—Ä–∏—É–ª—Ç ”©–≥–¥”©–≥ –±–æ–ª—Å–æ–Ω.
```

---

## ü§ñ Inference –∞—à–∏–≥–ª–∞—Ö –∫–æ–¥

```python
from unsloth import FastLanguageModel

# –ó–∞–≥–≤–∞—Ä—ã–≥ 4-bit-—ç—ç—Ä –∞—á–∞–∞–ª–∞—Ö
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="path/to/your/model",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True
)

# Inference –≥–æ—Ä–∏–º–¥ —à–∏–ª–∂“Ø“Ø–ª—ç—Ö
FastLanguageModel.for_inference(model)

alpaca_prompt = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{}

### Response:
{}"""

# Prompt-–∏–π–≥ —Ç–æ–∫–µ–Ω—á–∏–ª–∂ GPU-—Ä—É—É –¥–∞–º–∂—É—É–ª–∞—Ö
inputs = tokenizer([
    alpaca_prompt.format("–•–∏–π–º—ç–ª –æ—é—É–Ω —É—Ö–∞–∞–Ω –≥—ç–∂ —é—É –≤—ç?", "")
], return_tensors="pt").to("cuda")

# –¢–µ–∫—Å—Ç “Ø“Ø—Å–≥—ç—Ö
outputs = model.generate(
    **inputs,
    max_new_tokens=256,
    temperature=0.7,
    do_sample=True
)

print(tokenizer.batch_decode(outputs)[0])
```

---
> **–¢—ç–º–¥—ç–≥–ª—ç–ª:** –≠–Ω—ç –±–æ–ª —Å—É—Ä–≥–∞–ª—Ç—ã–Ω —Ç”©—Å”©–ª –±”©–≥”©”©–¥ —Ü—ç–≤—ç—Ä dataset + validation + testing-—ç—ç—Ä  
> “Ø—Ä–≥—ç–ª–∂–ª“Ø“Ø–ª—ç–Ω —Å–∞–π–∂—Ä—É—É–ª–±–∞–ª production-–¥ –±—ç–ª—ç–Ω –±–æ–ª–≥–æ—Ö –±–æ–ª–æ–º–∂—Ç–æ–π üöÄüá≤üá≥
