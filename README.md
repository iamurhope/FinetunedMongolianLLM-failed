# üá≤üá≥ Fine-tuned Mongolian LLM (Llama-2-7B + LoRA + Unsloth)

**Llama-2-7B —Å—É—É—Ä—å –∑–∞–≥–≤–∞—Ä—ã–≥ –º–æ–Ω–≥–æ–ª —Ö—ç–ª—ç—ç—Ä instruction –¥–∞–≥–∞—Ö, –∞—Å—É—É–ª—Ç-—Ö–∞—Ä–∏—É–ª—Ç –∑—ç—Ä—ç–≥ NLP –¥–∞–∞–ª–≥–∞–≤–∞—Ä—Ç –∑–æ—Ä–∏—É–ª–∂ Parameter-efficient (PEFT) –∞—Ä–≥–∞–∞—Ä LoRA –±–æ–ª–æ–Ω Unsloth –∞—à–∏–≥–ª–∞–Ω fine-tune —Ö–∏–π—Å—ç–Ω.**

---

## –û–Ω—Ü–ª–æ–≥

- **–°—É—É—Ä—å –∑–∞–≥–≤–∞—Ä:** Llama-2-7B (Meta AI)
- **–°—É—Ä–≥–∞–ª—Ç—ã–Ω –∞—Ä–≥–∞:** LoRA + 4-bit Quantization
- **Framework:** Unsloth (Colab optimized)
- **Dataset —Ñ–æ—Ä–º–∞—Ç:** Alpaca (`instruction`, `input`, `output`)
- **–ó–æ—Ä–∏–ª–≥–æ:** –ú–æ–Ω–≥–æ–ª —Ö—ç–ª–Ω–∏–π LLM-–Ω –∑–∞–∞–≤–∞—Ä –¥–∞–≥–∞—Ö –±–æ–ª–æ–Ω QA —á–∞–Ω–∞—Ä—ã–≥ —Å–∞–π–∂—Ä—É—É–ª–∞—Ö

---

## ‚öôÔ∏è –ó–∞–≥–≤–∞—Ä—ã–Ω —Ç–æ—Ö–∏—Ä–≥–æ–æ

```python
model_name = "meta-llama/Llama-2-7b-hf"
max_seq_length = 2048
lora_config = {
  "r": 16,
  "alpha": 16,
  "target_modules": [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
  ],
  "load_in_4bit": True
}
```

---

## –°—É—Ä–≥–∞–ª—Ç—ã–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä“Ø“Ø–¥

```python
training_args = {
  "per_device_train_batch_size": 2,
  "gradient_accumulation_steps": 4,
  "warmup_steps": 5,
  "num_train_epochs": 6,
  "learning_rate": 2e-4,
  "optimizer": "adamw_8bit",
  "weight_decay": 0.01,
  "lr_scheduler_type": "linear",
  "save_steps": 100,
  "save_total_limit": 3
}
```

## Dataset –∂–∏—à—ç—ç

```json
{
  "instruction": "–Ø—Ä–∏–∞ —Ç–∞–Ω–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π–≥ —Ö—ç—Ä—Ö—ç–Ω –∞—à–∏–≥–ª–∞–∂ –±–æ–ª–æ—Ö—ã–≥ 5 ”©–≥“Ø“Ø–ª–±—ç—Ä—ç—ç—Ä –∂–∏—à—ç—ç–ª–∂ ”©–≥”©”©—á?",
  "input": None,
  "output": "–î—É—É —Ç–∞–Ω–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π–≥ –º–∞—à–∏–Ω–¥ –≥–∞—Ä —á”©–ª”©”©—Ç—ç–π –∑–∞–ª–≥–∞—Ö –±–æ–ª–æ–º–∂–∏–π–≥ –æ–ª–≥–æ—Ö—ã–Ω —Ç—É–ª–¥ –∞—à–∏–≥–ª–∞–∂ –±–æ–ª–Ω–æ. –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–¥ –∑“Ø–≥—ç—ç—Ä –ª "–ñ–æ–Ω –î”©“Ø–≥ –¥—É—É–¥–Ω–∞ —É—É" –≥—ç–∂ —Ö—ç–ª—ç—Ö—ç–¥ –º–∞—à–∏–Ω –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä –¥—É—É–¥–ª–∞–≥–∞ —Ö–æ–ª–±–æ–Ω–æ. –¢“Ø“Ø–Ω—á–ª—ç–Ω –¥—É—É—Ç —Ç—É—à–∞–∞–ª—ã–≥ GPS –Ω–∞–≤–∏–≥–∞—Ü–∏ –∞—à–∏–≥–ª–∞—Ö, –∞–≥–∞–∞—Ä—ã–Ω —á–∞–Ω–∞—Ä—ã–≥ —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö, —Ö”©–≥–∂–∏–º —Å–æ–Ω—Å–æ—Ö–æ–¥ –∞—à–∏–≥–ª–∞–∂ –±–æ–ª–Ω–æ. –¶–∞–∞—à–∏–ª–±–∞–ª, —ç–Ω—ç –Ω—å Cortana –±–æ–ª–æ–Ω Alexa –∑—ç—Ä—ç–≥ –¥—É—É—Ç —Ç—É—Å–ª–∞—Ö—É—É–¥—ã–≥ –≥—ç—Ä –æ—Ä–æ–Ω –¥–∞—è–∞—Ä —Ç”©—Ö”©”©—Ä”©–º–∂“Ø“Ø–¥–∏–π–≥ —É–¥–∏—Ä–¥–∞—Ö–∞–¥ –∞—à–∏–≥–ª–∞–∂ –±–æ–ª–Ω–æ. –≠—Ü—ç—Å—Ç –Ω—å —Ö–∞–∞–ª–≥–∞ —Ç“Ø–≥–∂—ç—ç —Ç–∞–π–ª–∞—Ö —ç—Å–≤—ç–ª —Ç”©–ª–±”©—Ä –∑”©–≤—à”©”©—Ä”©—Ö”©–¥ –¥—É—É —Ö–æ–æ–ª–æ–π–¥ —Å—É—É—Ä–∏–ª—Å–∞–Ω –±–∏–æ–º–µ—Ç—Ä–∏–π–Ω –±–∞—Ç–∞–ª–≥–∞–∞–∂—É—É–ª–∞–ª—Ç–∞–¥ –∞—à–∏–≥–ª–∞–∂ –±–æ–ª–Ω–æ."
}
```

---

## ”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω–≥–∏–π–Ω —á–∞–Ω–∞—Ä—ã–Ω —Ç–æ–º –∞—Å—É—É–¥–∞–ª

| –ê—Å—É—É–¥–∞–ª | –¢–∞–π–ª–±–∞—Ä |
|---|---|
| `input` = float64 | –¢–µ–∫—Å—Ç –±–∞–π—Ö —ë—Å—Ç–æ–π —Ç–∞–ª–±–∞—Ä—Ç —Å–∞–Ω–∞–º—Å–∞—Ä–≥“Ø–π **—Ç–æ–æ** –æ—Ä—Å–æ–Ω |
| –ê–≤—Ç–æ –æ—Ä—á—É—É–ª–≥–∞ | –ê–Ω–≥–ª–∏‚Üí–ú–æ–Ω–≥–æ–ª –æ—Ä—á—É—É–ª–≥–∞ **–¥“Ø—Ä–º–∏–π–Ω –±–æ–ª–æ–Ω —É—Ç–≥—ã–Ω –∏—Ö –∞–ª–¥–∞–∞—Ç–∞–π** |
| Wrong facts | –¢“Ø“Ø—Ö, –≥–∞–∑–∞—Ä–∑“Ø–π, —à–∏–Ω–∂–ª—ç—Ö —É—Ö–∞–∞–Ω—ã **–±–∞—Ä–∏–º—Ç —Ö—É–¥–∞–ª/–∑”©—Ä—á–∏–ª—Ç—ç–π** |
| Mixed languages | –î–∞—Ç–∞–¥ **–º–æ–Ω–≥–æ–ª, –∞–Ω–≥–ª–∏, —Ö—è—Ç–∞–¥** —Ö–æ–ª–∏–≥–¥—Å–æ–Ω |
| Hallucination –∏—Ö | –•–∞—Ä–∏—É–ª—Ç—É—É–¥ –ª–æ–≥–∏–∫ –±–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç **—Ç–∞—Å–∞—Ä—Å–∞–Ω, —Ö–∏–π “Ø–∑—ç–≥–¥—ç–ª—Ç—ç–π** |

### –ù”©–ª”©”©:
```
   –°—É—É—Ä—å Llama-2-—ã–Ω –º—ç–¥–ª—ç–≥ –º—É—É –¥–∞—Ç–∞–¥ —Ö—ç—Ç overfit —Ö–∏–π—Å–Ω—ç—ç—Å  
   ‚ÄúCatastrophic Forgetting + Bad Overfitting‚Äù “Ø“Ø—Å—ç–∂, –±—É—Ä—É—É —Ö–∞—Ä–∏—É–ª—Ç ”©–≥–¥”©–≥ –±–æ–ª—Å–æ–Ω.
```
