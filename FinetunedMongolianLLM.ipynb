{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/Llama-2-7b-hf\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "uInX7_IOd6YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TrainerCallback\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "test_prompts = [\n",
        "    \"Хиймэл оюун гэж юу вэ?\",\n",
        "    \"Монголын нийслэл хот аль вэ?\",\n",
        "]\n",
        "\n",
        "class InferenceCallback(TrainerCallback):\n",
        "    def __init__(self, model, tokenizer, test_prompts, alpaca_prompt, inference_steps=100, log_file=\"inference_log.txt\"):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.test_prompts = test_prompts\n",
        "        self.alpaca_prompt = alpaca_prompt\n",
        "        self.inference_steps = inference_steps\n",
        "        self.log_file = log_file\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % self.inference_steps == 0:\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(f\"Running inference at step {state.global_step}\")\n",
        "            print(f\"{'='*80}\\n\")\n",
        "\n",
        "            self.model.eval()\n",
        "\n",
        "            for prompt in self.test_prompts:\n",
        "                inputs = self.tokenizer([\n",
        "                    self.alpaca_prompt.format(prompt, \"\")\n",
        "                ], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=128,\n",
        "                        use_cache=True,\n",
        "                        temperature=0.7,\n",
        "                        do_sample=True\n",
        "                    )\n",
        "\n",
        "                response = self.tokenizer.batch_decode(outputs)[0]\n",
        "                response_start = response.find(\"### Response:\") + len(\"### Response:\")\n",
        "                response_text = response[response_start:].strip()\n",
        "\n",
        "                print(f\"Prompt: {prompt}\")\n",
        "                print(f\"Response: {response_text}\\n\")\n",
        "                print(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "            self.model.train()\n",
        "\n",
        "        return control\n",
        "\n",
        "dataset = Dataset.from_file(\"/content/drive/MyDrive/llm/llm/data-00000-of-00001.arrow\")\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, output in zip(instructions, outputs):\n",
        "        text = alpaca_prompt.format(instruction, output) + tokenizer.eos_token\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "inference_callback = InferenceCallback(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    test_prompts=test_prompts,\n",
        "    alpaca_prompt=alpaca_prompt,\n",
        "    inference_steps=100\n",
        ")"
      ],
      "metadata": {
        "id": "Ke6014QoeDVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 6,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 100,\n",
        "        save_total_limit = 3,\n",
        "        load_best_model_at_end = False,\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "5S0VCfsKNqrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Хиймэл оюун гэж юу вэ?\",\n",
        "        \"\",\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        ")\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUeeyKLeeEzz",
        "outputId": "7092fcd6-cf50-4484-f2d8-2d27fbaa3723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Хиймэл оюун гэж юу вэ?\n",
            "\n",
            "### Response:\n",
            "Хиймэл оюун бол компьютерийн шинжлэх ухааны салбар бөгөөд өгөгдлөөс суралцаж, түүнийг ашиглах, урвуулахын тулд машин сургалт, байгалийн хэл боловсруулалт, өгөгдөл шинжилгээ зэрэг арга хэмжээг ашигладаг. Энэ нь хиймэл оюун мод, хиймэл оюун эсийн зэрэглэлийг хурдан, нарийвчлалтай илэрхийлэхэд анхаардаг.</s>\n"
          ]
        }
      ]
    }
  ]
}
